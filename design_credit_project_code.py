# -*- coding: utf-8 -*-
"""Design Credit Project Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13GQ4_hyVDV9uVnRsqrIb_ydrBt6HaUBc

#**Design Credit Project**

###**Dev Goel (B20CS090)**

##Importing the Necessary Python Libraries and Components
"""

import pandas as pd
import requests 
from bs4 import BeautifulSoup
import numpy as np
import warnings
warnings.filterwarnings("ignore")
from collections import Counter
import collections
from sklearn.feature_extraction.text import CountVectorizer
from yellowbrick.text import FreqDistVisualizer

"""#Social Media Apps

##Scrape Telegram Data
"""

def telegramData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

telegram_data, telegram_comments = telegramData("https://www.ipa4fun.com/history/15540/", 8)

print("- Before removing the duplicates from df:",telegram_data.shape)

# Removing the list brackets from bug fixes statements.
telegram_data['version_bugs_fixes'] = telegram_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
telegram_data = telegram_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
telegram_data['version_date'] = telegram_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",telegram_data.shape)

# Blank Line
print("")
telegram_data

telegram_comments = telegram_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(telegram_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in telegram_comments:
    if(any(email_service in i for email_service in L)):
        telegram_comments.remove(i)
print("After removing the '-' element from the list:",len(telegram_comments))

telegram_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(telegram_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(telegram_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_telegram_comments = list(set(telegram_comments))
len(unique_telegram_comments)
for i in range(10):
    print(unique_telegram_comments[i])

# telegram_data.to_csv("telegram_data.csv")

"""##Scrape Twitter Data"""

def twitterData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

twitter_data, twitter_comments = twitterData("https://www.ipa4fun.com/history/790/", 17)

twitter_data.shape

print("- Before removing the duplicates from df:", twitter_data.shape)

# Removing the list brackets from bug fixes statements.
twitter_data['version_bugs_fixes'] = twitter_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
twitter_data = twitter_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
twitter_data['version_date'] = twitter_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",twitter_data.shape)

# Blank Line
print("")
twitter_data

twitter_comments = twitter_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(twitter_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in twitter_comments:
    if(any(email_service in i for email_service in L)):
        twitter_comments.remove(i)
print("After removing the '-' element from the list:",len(twitter_comments))

twitter_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(twitter_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(twitter_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_twitter_comments = list(set(twitter_comments))
len(unique_twitter_comments)
for i in range(16,20):
    print(unique_twitter_comments[i])

# twitter_data.to_csv("twitter_data.csv")

"""##Scrape Facebook Data"""

def facebookData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

facebook_data, facebook_comments = facebookData("https://www.ipa4fun.com/history/704/", 17)

facebook_data.shape

print("-> Before removing the duplicates from df:",facebook_data.shape)

# Removing the list brackets from bug fixes statements.
facebook_data['version_bugs_fixes'] = facebook_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
facebook_data = facebook_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
facebook_data['version_date'] = facebook_data['version_date'].apply(updateDate)

print("-> After removing the duplicates from df:",facebook_data.shape)

# Blank Line
print("")
facebook_data

facebook_comments = facebook_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(facebook_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-', 'itunes', 'ios7']
for i in facebook_comments:
    if(any(email_service in i for email_service in L)):
        facebook_comments.remove(i)
print("After removing the '-' element from the list:",len(facebook_comments))

facebook_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(facebook_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(facebook_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v', n=30)
visualizer.fit(docs)
visualizer.show()

unique_facebook_comments = list(set(facebook_comments))
print(unique_facebook_comments[2])
print(unique_facebook_comments[3])

# facebook_data.to_csv("facebook_data.csv")

"""##Scrape Instagram Data"""

def instagramData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

instagram_data, instagram_comments = instagramData("https://www.ipa4fun.com/history/871/", 18)

instagram_data.shape

print("- Before removing the duplicates from df:",instagram_data.shape)

# Removing the list brackets from bug fixes statements.
instagram_data['version_bugs_fixes'] = instagram_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
instagram_data = instagram_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
instagram_data['version_date'] = instagram_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",instagram_data.shape)

# Blank Line
print("")
instagram_data

instagram_comments = instagram_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(instagram_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in instagram_comments:
    if(any(email_service in i for email_service in L)):
        instagram_comments.remove(i)
print("After removing the '-' element from the list:",len(instagram_comments))

instagram_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(instagram_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(instagram_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_instagram_comments = list(set(instagram_comments))
len(unique_instagram_comments)
for i in range(10):
    print(unique_instagram_comments[i])

# instagram_data.to_csv("instagram_data.csv")

"""##Scrape WhatsApp Messenger Data"""

def whatsappData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

whatsapp_data, whatsapp_comments = whatsappData("https://www.ipa4fun.com/history/278/", 11)

whatsapp_data.shape

print("- Before removing the duplicates from df:",whatsapp_data.shape)

# Removing the list brackets from bug fixes statements.
whatsapp_data['version_bugs_fixes'] = whatsapp_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
whatsapp_data = whatsapp_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
whatsapp_data['version_date'] = whatsapp_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",whatsapp_data.shape)

# Blank Line
print("")
whatsapp_data

whatsapp_comments = whatsapp_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(whatsapp_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphones', 'iphone', '-','Iphone']
for i in whatsapp_comments:
    if(any(email_service in i for email_service in L)):
        whatsapp_comments.remove(i)
print("After removing the '-' element from the list:",len(whatsapp_comments))

whatsapp_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(whatsapp_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(whatsapp_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_whatsapp_comments = list(set(whatsapp_comments))
len(unique_whatsapp_comments)
for i in range(10):
    print(unique_whatsapp_comments[i])

# whatsapp_data.to_csv("whatsapp_data.csv")

"""#Educational Apps

##Scrape Chegg Data
"""

def cheggData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

chegg_data, chegg_comments = cheggData("https://www.ipa4fun.com/history/4473/", 8)

chegg_data.shape

print("- Before removing the duplicates from df:",chegg_data.shape)

# Removing the list brackets from bug fixes statements.
chegg_data['version_bugs_fixes'] = chegg_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
chegg_data = chegg_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
chegg_data['version_date'] = chegg_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",chegg_data.shape)

# Blank Line
print("")
chegg_data

chegg_comments = chegg_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(chegg_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in chegg_comments:
    if(any(email_service in i for email_service in L)):
        chegg_comments.remove(i)
print("After removing the '-' element from the list:",len(chegg_comments))

chegg_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(chegg_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(chegg_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_chegg_comments = list(set(chegg_comments))
len(unique_chegg_comments)
for i in range(10):
    print(unique_chegg_comments[i])

# chegg_data.to_csv("chegg_data.csv")

"""##Scrape Quizlet Flashcards Data"""

def quizletFlashcardsData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

quizletFlashcards_data, quizletFlashcards_comments = quizletFlashcardsData("https://www.ipa4fun.com/history/507/", 11)

quizletFlashcards_data.shape

print("- Before removing the duplicates from df:",quizletFlashcards_data.shape)

# Removing the list brackets from bug fixes statements.
quizletFlashcards_data['version_bugs_fixes'] = quizletFlashcards_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
quizletFlashcards_data = quizletFlashcards_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
quizletFlashcards_data['version_date'] = quizletFlashcards_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",quizletFlashcards_data.shape)

# Blank Line
print("")
quizletFlashcards_data

quizletFlashcards_comments = quizletFlashcards_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(quizletFlashcards_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in quizletFlashcards_comments:
    if(any(email_service in i for email_service in L)):
        quizletFlashcards_comments.remove(i)
print("After removing the '-' element from the list:",len(quizletFlashcards_comments))

quizletFlashcards_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(quizletFlashcards_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(quizletFlashcards_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_quizletFlashcards_comments = list(set(quizletFlashcards_comments))
len(unique_quizletFlashcards_comments)
for i in range(10):
    print(unique_quizletFlashcards_comments[i])

# quizletFlashcards_data.to_csv("quizletFlashcards_data.csv")

"""##Scrape Udemy Data"""

def udemyData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

udemy_data, udemy_comments = udemyData("https://www.ipa4fun.com/history/109/", 10)

udemy_data.shape

print("- Before removing the duplicates from df:",udemy_data.shape)

# Removing the list brackets from bug fixes statements.
udemy_data['version_bugs_fixes'] = udemy_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
udemy_data = udemy_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
udemy_data['version_date'] = udemy_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",udemy_data.shape)

# Blank Line
print("")
udemy_data

udemy_comments = udemy_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(udemy_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in udemy_comments:
    if(any(email_service in i for email_service in L)):
        udemy_comments.remove(i)
print("After removing the '-' element from the list:",len(udemy_comments))

udemy_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(udemy_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(udemy_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_udemy_comments = list(set(udemy_comments))
len(unique_udemy_comments)
for i in range(10):
    print(unique_udemy_comments[i])

# udemy_data.to_csv("udemy_data.csv")

"""##Scrape Course Hero Data"""

def courseHeroData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

courseHero_data, courseHero_comments = courseHeroData("https://www.ipa4fun.com/history/19451/", 8)

courseHero_data.shape

print("- Before removing the duplicates from df:",courseHero_data.shape)

# Removing the list brackets from bug fixes statements.
courseHero_data['version_bugs_fixes'] = courseHero_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
courseHero_data = courseHero_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
courseHero_data['version_date'] = courseHero_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",courseHero_data.shape)

# Blank Line
print("")
courseHero_data

courseHero_comments = courseHero_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(courseHero_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in courseHero_comments:
    if(any(email_service in i for email_service in L)):
        courseHero_comments.remove(i)
print("After removing the '-' element from the list:",len(courseHero_comments))

courseHero_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(courseHero_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(courseHero_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v', n=40)
visualizer.fit(docs)
visualizer.show()

unique_courseHero_comments = list(set(courseHero_comments))
len(unique_courseHero_comments)
for i in range(8):
    print(unique_courseHero_comments[i])

# courseHero_data.to_csv("courseHero_data.csv")

"""##Scrape Photo Math Data"""

def photoMathData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

photoMath_data, photoMath_comments = photoMathData("https://www.ipa4fun.com/history/7122/", 6)

photoMath_data.shape

print("- Before removing the duplicates from df:",photoMath_data.shape)

# Removing the list brackets from bug fixes statements.
photoMath_data['version_bugs_fixes'] = photoMath_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
photoMath_data = photoMath_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
photoMath_data['version_date'] = photoMath_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",photoMath_data.shape)

# Blank Line
print("")
photoMath_data

photoMath_comments = photoMath_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(photoMath_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in photoMath_comments:
    if(any(email_service in i for email_service in L)):
        photoMath_comments.remove(i)
print("After removing the '-' element from the list:",len(photoMath_comments))

photoMath_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(photoMath_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(photoMath_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v', n = 25)
visualizer.fit(docs)
visualizer.show()

unique_photoMath_comments = list(set(photoMath_comments))
len(unique_photoMath_comments)
for i in range(8):
    print(unique_photoMath_comments[i])

# photoMath_data.to_csv("photoMath_data.csv")

"""#Gaming Apps

##Scrape Ludo King Data
"""

def ludoKingData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

ludoKing_data, ludoKing_comments = ludoKingData("https://www.ipa4fun.com/history/284819/", 2)

ludoKing_data.shape

print("- Before removing the duplicates from df:",ludoKing_data.shape)

# Removing the list brackets from bug fixes statements.
ludoKing_data['version_bugs_fixes'] = ludoKing_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
ludoKing_data = ludoKing_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
ludoKing_data['version_date'] = ludoKing_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",ludoKing_data.shape)

# Blank Line
print("")
ludoKing_data

ludoKing_comments = ludoKing_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(ludoKing_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in ludoKing_comments:
    if(any(email_service in i for email_service in L)):
        ludoKing_comments.remove(i)
print("After removing the '-' element from the list:",len(ludoKing_comments))

ludoKing_comments

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(ludoKing_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(ludoKing_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v', n=15)
visualizer.fit(docs)
visualizer.show()

unique_ludoKing_comments = list(set(ludoKing_comments))
len(unique_ludoKing_comments)
for i in range(5):
    print(unique_ludoKing_comments[i])

# ludoKing_data.to_csv("ludoKing_data.csv")

"""##Scrape Clash of Clans Data"""

def clashOfClansData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

clashOfClans_data, clashOfClans_comments = clashOfClansData("https://www.ipa4fun.com/history/606/", 7)

clashOfClans_data.shape

print("- Before removing the duplicates from df:",clashOfClans_data.shape)

# Removing the list brackets from bug fixes statements.
clashOfClans_data['version_bugs_fixes'] = clashOfClans_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
clashOfClans_data = clashOfClans_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
clashOfClans_data['version_date'] = clashOfClans_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",clashOfClans_data.shape)

# Blank Line
print("")
clashOfClans_data

clashOfClans_comments = clashOfClans_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(clashOfClans_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in clashOfClans_comments:
    if(any(email_service in i for email_service in L)):
        clashOfClans_comments.remove(i)
print("After removing the '-' element from the list:",len(clashOfClans_comments))

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(clashOfClans_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(clashOfClans_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_clashOfClans_comments = list(set(clashOfClans_comments))
len(unique_clashOfClans_comments)
for i in range(10):
    print(unique_clashOfClans_comments[i])

# clashOfClans_data.to_csv("clashOfClans_data.csv")

"""##Scrape Asphalt 8: Airborne Data"""

def asphaltAirborneData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

asphaltAirborne_data, asphaltAirborne_comments = asphaltAirborneData("https://www.ipa4fun.com/history/44/", 4)

asphaltAirborne_data.shape

print("- Before removing the duplicates from df:",asphaltAirborne_data.shape)

# Removing the list brackets from bug fixes statements.
asphaltAirborne_data['version_bugs_fixes'] = asphaltAirborne_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
asphaltAirborne_data = asphaltAirborne_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
asphaltAirborne_data['version_date'] = asphaltAirborne_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",asphaltAirborne_data.shape)

# Blank Line
print("")
asphaltAirborne_data

asphaltAirborne_comments = asphaltAirborne_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(asphaltAirborne_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in asphaltAirborne_comments:
    if(any(email_service in i for email_service in L)):
        asphaltAirborne_comments.remove(i)
print("After removing the '-' element from the list:",len(asphaltAirborne_comments))

asphaltAirborne_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(asphaltAirborne_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(asphaltAirborne_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_asphaltAirborne_comments = list(set(asphaltAirborne_comments))
len(unique_asphaltAirborne_comments)
for i in range(8):
    print(unique_asphaltAirborne_comments[i])

# asphaltAirborne_data.to_csv("asphaltAirborne_data.csv")

"""##Scrape Candy Crush Saga Data"""

def candyCrushData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

candyCrush_data, candyCrush_comments = candyCrushData("https://www.ipa4fun.com/history/294/", 10)

candyCrush_data.shape

print("- Before removing the duplicates from df:",candyCrush_data.shape)

# Removing the list brackets from bug fixes statements.
candyCrush_data['version_bugs_fixes'] = candyCrush_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
candyCrush_data = candyCrush_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
candyCrush_data['version_date'] = candyCrush_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",candyCrush_data.shape)

# Blank Line
print("")
candyCrush_data

candyCrush_comments = candyCrush_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(candyCrush_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in candyCrush_comments:
    if(any(email_service in i for email_service in L)):
        candyCrush_comments.remove(i)
print("After removing the '-' element from the list:",len(candyCrush_comments))

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(candyCrush_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(candyCrush_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_candyCrush_comments = list(set(candyCrush_comments))
len(unique_candyCrush_comments)
for i in range(10):
    print(unique_candyCrush_comments[i])

# candyCrush_data.to_csv("candyCrush_data.csv")

"""##Scrape Hill Climb Racing Data"""

def hillClimbRacingData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

hillClimbRacing_data, hillClimbRacing_comments = hillClimbRacingData("https://www.ipa4fun.com/history/21/", 3)

hillClimbRacing_data.shape

print("- Before removing the duplicates from df:",hillClimbRacing_data.shape)

# Removing the list brackets from bug fixes statements.
hillClimbRacing_data['version_bugs_fixes'] = hillClimbRacing_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
hillClimbRacing_data = hillClimbRacing_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
hillClimbRacing_data['version_date'] = hillClimbRacing_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",hillClimbRacing_data.shape)

# Blank Line
print("")
hillClimbRacing_data

hillClimbRacing_comments = hillClimbRacing_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(hillClimbRacing_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in hillClimbRacing_comments:
    if(any(email_service in i for email_service in L)):
        hillClimbRacing_comments.remove(i)
print("After removing the '-' element from the list:",len(hillClimbRacing_comments))

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(hillClimbRacing_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(hillClimbRacing_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v', n=15)
visualizer.fit(docs)
visualizer.show()

unique_hillClimbRacing_comments = list(set(hillClimbRacing_comments))
len(unique_hillClimbRacing_comments)
for i in range(8):
    print(unique_hillClimbRacing_comments[i])

# hillClimbRacing_data.to_csv("hillClimbRacing_data.csv")

"""#Utility Apps

##Scrape Google Maps Data
"""

def googleMapsData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

googleMaps_data, googleMaps_comments = googleMapsData("https://www.ipa4fun.com/history/39/", 9)

googleMaps_data.shape

print("- Before removing the duplicates from df:",googleMaps_data.shape)

# Removing the list brackets from bug fixes statements.
googleMaps_data['version_bugs_fixes'] = googleMaps_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
googleMaps_data = googleMaps_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
googleMaps_data['version_date'] = googleMaps_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",googleMaps_data.shape)

# Blank Line
print("")
googleMaps_data

googleMaps_comments = googleMaps_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(googleMaps_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in googleMaps_comments:
    if(any(email_service in i for email_service in L)):
        googleMaps_comments.remove(i)
print("After removing the '-' element from the list:",len(googleMaps_comments))

googleMaps_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(googleMaps_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(googleMaps_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_googleMaps_comments = list(set(googleMaps_comments))
len(unique_googleMaps_comments)
for i in range(10):
    print(unique_googleMaps_comments[i])

# googleMaps_data.to_csv("googleMaps_data.csv")

"""##Scrape Google Translate Data"""

def googleTranslateData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

googleTranslate_data, googleTranslate_comments = googleTranslateData("https://www.ipa4fun.com/history/343/", 4)

googleTranslate_data.shape

print("- Before removing the duplicates from df:",googleTranslate_data.shape)

# Removing the list brackets from bug fixes statements.
googleTranslate_data['version_bugs_fixes'] = googleTranslate_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
googleTranslate_data = googleTranslate_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
googleTranslate_data['version_date'] = googleTranslate_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",googleTranslate_data.shape)

# Blank Line
print("")
googleTranslate_data

googleTranslate_comments = googleTranslate_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(googleTranslate_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in googleTranslate_comments:
    if(any(email_service in i for email_service in L)):
        googleTranslate_comments.remove(i)
print("After removing the '-' element from the list:",len(googleTranslate_comments))

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(googleTranslate_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(googleTranslate_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v', n = 40)
visualizer.fit(docs)
visualizer.show()

unique_googleTranslate_comments = list(set(googleTranslate_comments))
len(unique_googleTranslate_comments)
for i in range(8):
    print(unique_googleTranslate_comments[i])

# googleTranslate_data.to_csv("googleTranslate_data.csv")

"""##Scrape Microsoft Lens Data"""

def microsoftLensData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

microsoftLens_data, microsoftLens_comments = microsoftLensData("https://www.ipa4fun.com/history/21686/", 4)

microsoftLens_data.shape

print("- Before removing the duplicates from df:",microsoftLens_data.shape)

# Removing the list brackets from bug fixes statements.
microsoftLens_data['version_bugs_fixes'] = microsoftLens_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
microsoftLens_data = microsoftLens_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
microsoftLens_data['version_date'] = microsoftLens_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",microsoftLens_data.shape)

# Blank Line
print("")
microsoftLens_data

microsoftLens_comments = microsoftLens_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(microsoftLens_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in microsoftLens_comments:
    if(any(email_service in i for email_service in L)):
        microsoftLens_comments.remove(i)
print("After removing the '-' element from the list:",len(microsoftLens_comments))

microsoftLens_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(microsoftLens_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(microsoftLens_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v', n = 10)
visualizer.fit(docs)
visualizer.show()

unique_microsoftLens_comments = list(set(microsoftLens_comments))
len(unique_microsoftLens_comments)
for i in range(4):
    print(unique_microsoftLens_comments[i])

# microsoftLens_data.to_csv("microsoftLens_data.csv")

"""##Scrape Truecaller Data"""

def truecallerData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

truecaller_data, truecaller_comments = truecallerData("https://www.ipa4fun.com/history/22656/", 7)

truecaller_data.shape

print("- Before removing the duplicates from df:",truecaller_data.shape)

# Removing the list brackets from bug fixes statements.
truecaller_data['version_bugs_fixes'] = truecaller_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
truecaller_data = truecaller_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
truecaller_data['version_date'] = truecaller_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",truecaller_data.shape)

# Blank Line
print("")
truecaller_data

truecaller_comments = truecaller_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(truecaller_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in truecaller_comments:
    if(any(email_service in i for email_service in L)):
        truecaller_comments.remove(i)
print("After removing the '-' element from the list:",len(truecaller_comments))

truecaller_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(truecaller_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(truecaller_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_truecaller_comments = list(set(truecaller_comments))
len(unique_truecaller_comments)
for i in range(3,10):
    print(unique_truecaller_comments[i])

# truecaller_data.to_csv("truecaller_data.csv")

"""##Scrape Xender Data"""

def xenderData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

xender_data, xender_comments = xenderData("https://www.ipa4fun.com/history/218493/", 2)

xender_data.shape

print("- Before removing the duplicates from df:",xender_data.shape)

# Removing the list brackets from bug fixes statements.
xender_data['version_bugs_fixes'] = xender_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
xender_data = xender_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
xender_data['version_date'] = xender_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",xender_data.shape)

# Blank Line
print("")
xender_data

xender_comments = xender_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(xender_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in xender_comments:
    if(any(email_service in i for email_service in L)):
        xender_comments.remove(i)
print("After removing the '-' element from the list:",len(xender_comments))

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(xender_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(xender_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v', n=25)
visualizer.fit(docs)
visualizer.show()

unique_xender_comments = list(set(xender_comments))
len(unique_xender_comments)
for i in range(7):
    print(unique_xender_comments[i])

# xender_data.to_csv("xender_data.csv")

"""#Finance/Banking Apps

##Scrape Paypal Data
"""

def paypalData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U' and x[1] != 'S'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

paypal_data, paypal_comments = paypalData("https://www.ipa4fun.com/history/1015/", 9)

paypal_data.shape

print("- Before removing the duplicates from df:",paypal_data.shape)

# Removing the list brackets from bug fixes statements.
paypal_data['version_bugs_fixes'] = paypal_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
paypal_data = paypal_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
paypal_data['version_date'] = paypal_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",paypal_data.shape)

# Blank Line
print("")
paypal_data

paypal_comments = paypal_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(paypal_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in paypal_comments:
    if(any(email_service in i for email_service in L)):
        paypal_comments.remove(i)
print("After removing the '-' element from the list:",len(paypal_comments))

paypal_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(paypal_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(paypal_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_paypal_comments = list(set(paypal_comments))
len(unique_paypal_comments)
for i in range(10):
    print(unique_paypal_comments[i])

# paypal_data.to_csv("paypal_data.csv")

"""##Scrape Amex Bank Data"""

def amexData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

amex_data, amex_comments = amexData("https://www.ipa4fun.com/history/838/", 6)

amex_data.shape

print("- Before removing the duplicates from df:",amex_data.shape)

# Removing the list brackets from bug fixes statements.
amex_data['version_bugs_fixes'] = amex_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
amex_data = amex_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
amex_data['version_date'] = amex_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",amex_data.shape)

# Blank Line
print("")
amex_data

amex_comments = amex_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(amex_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in amex_comments:
    if(any(email_service in i for email_service in L)):
        amex_comments.remove(i)
print("After removing the '-' element from the list:",len(amex_comments))

amex_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(amex_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(amex_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_amex_comments = list(set(amex_comments))
len(unique_amex_comments)
for i in range(10):
    print(unique_amex_comments[i])

# amex_data.to_csv("amex_data.csv")

"""##Scrape Blockchain.com Data"""

def blockchainData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U' and x[1] != 'S'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

blockchain_data, blockchain_comments = blockchainData("https://www.ipa4fun.com/history/277353/", 7)

blockchain_data.shape

print("- Before removing the duplicates from df:",blockchain_data.shape)

# Removing the list brackets from bug fixes statements.
blockchain_data['version_bugs_fixes'] = blockchain_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
blockchain_data = blockchain_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
blockchain_data['version_date'] = blockchain_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",blockchain_data.shape)

# Blank Line
print("")
blockchain_data

blockchain_comments = blockchain_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(blockchain_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in blockchain_comments:
    if(any(email_service in i for email_service in L)):
        blockchain_comments.remove(i)
print("After removing the '-' element from the list:",len(blockchain_comments))

blockchain_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(blockchain_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(blockchain_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_blockchain_comments = list(set(blockchain_comments))
len(unique_blockchain_comments)
for i in range(10):
    print(unique_blockchain_comments[i])

# blockchain_data.to_csv("blockchain_data.csv")

"""##Scrape Coinbase Data"""

def coinbaseData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

coinbase_data, coinbase_comments = coinbaseData("https://www.ipa4fun.com/history/132841/", 12)

coinbase_data.shape

print("- Before removing the duplicates from df:",coinbase_data.shape)

# Removing the list brackets from bug fixes statements.
coinbase_data['version_bugs_fixes'] = coinbase_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
coinbase_data = coinbase_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
coinbase_data['version_date'] = coinbase_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",coinbase_data.shape)

# Blank Line
print("")
coinbase_data

coinbase_comments = coinbase_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(coinbase_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'ios', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in coinbase_comments:
    if(any(email_service in i for email_service in L)):
        coinbase_comments.remove(i)
print("After removing the '-' element from the list:",len(coinbase_comments))

coinbase_comments

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(coinbase_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(coinbase_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v', n=30)
visualizer.fit(docs)
visualizer.show()

unique_coinbase_comments = list(set(coinbase_comments))
len(unique_coinbase_comments)
for i in range(2):
    print(unique_coinbase_comments[i])

# coinbase_data.to_csv("coinbase_data.csv")

"""##Scrape Yahoo Finance Data"""

def yahooFinanceData(main_url, num_of_sites):
    urls = []
    version_name = []
    version_date = []
    version_comments = []
    all_comments = []
    for i in range(1, num_of_sites+1):
        url = main_url + str(i) + '/'
        urls.append(url)
    for i in range(num_of_sites):
        response = requests.get(urls[i])
        print("Website:", i+1, "Response:", response)
        soup = BeautifulSoup(response.content, 'html.parser')
        version_data = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        data_lst = []
        for i in version_data:
            if(i.a != None):
                data_lst.append(i.a.text)
                version_name.append(i.a.text)
        a = soup.find('ol', attrs = {'class': 'history'}).find_all("li")
        a = np.array(a)
        z = [str(i) for i in a]
        dates = []
        for i in z:
            x = i[i.find('(')+1:i.find(')')]
            if(x[0] == 'U'):
                dates.append(x)
                version_date.append(x)
        LL = []
        com = soup.find_all('ul', attrs = {'class': 'changes'})
        for j in com:
            L = []
            text = list(j.descendants)
            for i in range(2, len(text), 3):
                L.append(text[i])
                all_comments.append(text[i])
            LL.append(L)
            version_comments.append(L)
        val = len(data_lst)-len(LL)
        for i in range(val):
            version_comments.append("-")
    print(len(version_name))
    print(len(version_date))
    print(len(version_comments))
    df = pd.DataFrame({'version_name': version_name, 'version_date': version_date, 'version_bugs_fixes': version_comments})
    return df, all_comments

yahooFinance_data, yahooFinance_comments = yahooFinanceData("https://www.ipa4fun.com/history/1327/", 10)

yahooFinance_data.shape

print("- Before removing the duplicates from df:",yahooFinance_data.shape)

# Removing the list brackets from bug fixes statements.
yahooFinance_data['version_bugs_fixes'] = yahooFinance_data['version_bugs_fixes'].str[0]

# Removing the duplicates from the dataframe.
yahooFinance_data = yahooFinance_data.drop_duplicates()

# Updating the date format.
def updateDate(x):
	return x.replace("Updated: ","")
yahooFinance_data['version_date'] = yahooFinance_data['version_date'].apply(updateDate)

print("- After removing the duplicates from df:",yahooFinance_data.shape)

# Blank Line
print("")
yahooFinance_data

yahooFinance_comments = yahooFinance_data['version_bugs_fixes'].tolist()
print("Before removing the '-' element from the list:",len(yahooFinance_comments))

# Removing the random words from the dataset/dataframe.
L = ['Apple', 'IOS', 'iOS', 'apple', 'IOS', 'Iphone', 'iphone', '-']
for i in yahooFinance_comments:
    if(any(email_service in i for email_service in L)):
        yahooFinance_comments.remove(i)
print("After removing the '-' element from the list:",len(yahooFinance_comments))

vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(yahooFinance_comments)
vectorizer.get_feature_names_out()

vectorizer = CountVectorizer(stop_words='english')
docs       = vectorizer.fit_transform(yahooFinance_comments)
features   = vectorizer.get_feature_names()
visualizer = FreqDistVisualizer(features=features, orient='v')
visualizer.fit(docs)
visualizer.show()

unique_yahooFinance_comments = list(set(yahooFinance_comments))
len(unique_yahooFinance_comments)
for i in range(10):
    print(unique_yahooFinance_comments[i])

# yahooFinance_data.to_csv("yahooFinance_data.csv")